{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52572527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "FOLDER_PATH = \"./documents\"\n",
    "\n",
    "def build_inverted_index(folder_path):\n",
    "    doc_tokens = {}\n",
    "    doc_id_map = {}\n",
    "    doc_counter = 1\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in sorted(files):\n",
    "            path = os.path.join(root, file_name)\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                tokens = set()\n",
    "                for line in f:\n",
    "                    words = line.lower().split()\n",
    "                    tokens.update(words)\n",
    "            doc_tokens[str(doc_counter)] = tokens\n",
    "            doc_id_map[str(doc_counter)] = file_name\n",
    "            doc_counter += 1\n",
    "\n",
    "    inverted_index = defaultdict(lambda: {'docs': [], 'doc_freq': 0})\n",
    "\n",
    "    for doc_id, words in doc_tokens.items():\n",
    "        for word in words:\n",
    "            inverted_index[word]['docs'].append(int(doc_id))\n",
    "            inverted_index[word]['doc_freq'] += 1\n",
    "\n",
    "    return inverted_index, doc_id_map, set(range(1, doc_counter))\n",
    "\n",
    "def evaluate_boolean_query(query, inverted_index, all_docs):\n",
    "    tokens = query.upper().split()\n",
    "    postfix = infix_to_postfix(tokens)\n",
    "    result = evaluate_postfix(postfix, inverted_index, all_docs)\n",
    "    return sorted(result)\n",
    "\n",
    "\n",
    "def infix_to_postfix(tokens):\n",
    "    precedence = {'NOT': 3, 'AND': 2, 'OR': 1}\n",
    "    output = []\n",
    "    stack = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in ('AND', 'OR', 'NOT'):\n",
    "            output.append(token.lower())\n",
    "        elif token == 'NOT':\n",
    "            stack.append(token)\n",
    "        else:\n",
    "            while stack and precedence.get(stack[-1], 0) >= precedence[token]:\n",
    "                output.append(stack.pop())\n",
    "            stack.append(token)\n",
    "\n",
    "    while stack:\n",
    "        output.append(stack.pop())\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def and_operation_with_skip(list1, list2):\n",
    "    result = []\n",
    "    n1, n2 = len(list1), len(list2)\n",
    "    skip1 = int(math.sqrt(n1)) if n1 > 0 else 0\n",
    "    skip2 = int(math.sqrt(n2)) if n2 > 0 else 0\n",
    "    i, j = 0, 0\n",
    "\n",
    "    while i < n1 and j < n2:\n",
    "        if list1[i] == list2[j]:\n",
    "            result.append(list1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif list1[i] < list2[j]:\n",
    "            next_i = i + skip1 if (i + skip1 < n1) else n1 - 1\n",
    "            if skip1 > 1 and list1[next_i] <= list2[j]:\n",
    "                while next_i < n1 and list1[next_i] <= list2[j]:\n",
    "                    i = next_i\n",
    "                    next_i = i + skip1 if (i + skip1 < n1) else n1 - 1\n",
    "                if list1[i] < list2[j]:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        else:\n",
    "            next_j = j + skip2 if (j + skip2 < n2) else n2 - 1\n",
    "            if skip2 > 1 and list2[next_j] <= list1[i]:\n",
    "                while next_j < n2 and list2[next_j] <= list1[i]:\n",
    "                    j = next_j\n",
    "                    next_j = j + skip2 if (j + skip2 < n2) else n2 - 1\n",
    "                if list2[j] < list1[i]:\n",
    "                    j += 1\n",
    "            else:\n",
    "                j += 1\n",
    "    return result\n",
    "\n",
    "def and_operation(list1, list2):\n",
    "    result = []\n",
    "    i, j = 0, 0\n",
    "    while i < len(list1) and j < len(list2):\n",
    "        if list1[i] == list2[j]:\n",
    "            result.append(list1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif list1[i] < list2[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return result\n",
    "\n",
    "def or_operation(list1, list2):\n",
    "    result = []\n",
    "    i, j = 0, 0\n",
    "    while i < len(list1) and j < len(list2):\n",
    "        if list1[i] == list2[j]:\n",
    "            result.append(list1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif list1[i] < list2[j]:\n",
    "            result.append(list1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(list2[j])\n",
    "            j += 1\n",
    "    # Add remaining elements\n",
    "    result.extend(list1[i:])\n",
    "    result.extend(list2[j:])\n",
    "    return result\n",
    "\n",
    "def evaluate_postfix(postfix, inverted_index, all_docs):\n",
    "    stack = []\n",
    "    all_docs_list = sorted(list(all_docs))\n",
    "    for token in postfix:\n",
    "        if token not in ('AND', 'OR', 'NOT'):\n",
    "            docs = sorted(inverted_index[token]['docs']) if token in inverted_index else []\n",
    "            stack.append(docs)\n",
    "        elif token == 'NOT':\n",
    "            operand = stack.pop()\n",
    "            result = [doc for doc in all_docs_list if doc not in set(operand)]\n",
    "            stack.append(result)\n",
    "        else:\n",
    "            right = stack.pop()\n",
    "            left = stack.pop()\n",
    "            if token == 'AND':\n",
    "                stack.append(and_operation(left, right))\n",
    "            elif token == 'OR':\n",
    "                stack.append(or_operation(left, right))\n",
    "    return stack.pop() if stack else []\n",
    "\n",
    "\n",
    "def term_document_frequency_matrix(inverted_index, doc_id_map, folder_path):\n",
    "    terms = sorted(inverted_index.keys())\n",
    "    docs = [doc_id_map[str(i)] for i in range(1, len(doc_id_map) + 1)]\n",
    "\n",
    "    doc_word_counts = {}\n",
    "    for doc_id, file_name in doc_id_map.items():\n",
    "        path = os.path.join(folder_path, file_name)\n",
    "        word_counts = {}\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                for word in line.lower().split():\n",
    "                    word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        doc_word_counts[int(doc_id)] = word_counts\n",
    "\n",
    "    matrix = []\n",
    "    for term in terms:\n",
    "        row = []\n",
    "        for i in range(1, len(doc_id_map) + 1):\n",
    "            row.append(doc_word_counts[i].get(term, 0))\n",
    "        matrix.append(row)\n",
    "    df = pd.DataFrame(matrix, index=terms, columns=docs)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "764cd1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: accelerates AND stress\n",
      "Matched Doc IDs: [5]\n",
      "\n",
      "Query: biology OR science\n",
      "Matched Doc IDs: [2, 3, 5]\n",
      "\n",
      "Query: NOT stress\n",
      "Matched Doc IDs: [1, 2, 3, 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inverted_index, doc_id_map, all_docs = build_inverted_index(FOLDER_PATH)\n",
    "\n",
    "df = term_document_frequency_matrix(inverted_index, doc_id_map,folder_path=FOLDER_PATH)\n",
    "# print(df.head(50))\n",
    "\n",
    "queries = [\n",
    "    \"accelerates AND stress\",\n",
    "    \"biology OR science\",\n",
    "    \"NOT stress\",\n",
    "\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    result = evaluate_boolean_query(q, inverted_index, all_docs)\n",
    "    print(f\"Query: {q}\\nMatched Doc IDs: {result}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3398930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumped</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brown  dog  fox  jumped  lazy  over  quick  the\n",
       "0      1    0    1       0     0     0      1    1\n",
       "1      0    1    0       1     1     1      0    1\n",
       "2      0    0    1       0     0     0      0    1\n",
       "3      0    3    0       0     0     0      0    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumped</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brown   dog   fox  jumped  lazy  over  quick   the\n",
       "0   0.25  0.00  0.25     0.0   0.0   0.0   0.25  0.25\n",
       "1   0.00  0.20  0.00     0.2   0.2   0.2   0.00  0.20\n",
       "2   0.00  0.00  0.50     0.0   0.0   0.0   0.00  0.50\n",
       "3   0.00  0.75  0.00     0.0   0.0   0.0   0.00  0.25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "brown     1.916291\n",
       "dog       1.510826\n",
       "fox       1.510826\n",
       "jumped    1.916291\n",
       "lazy      1.916291\n",
       "over      1.916291\n",
       "quick     1.916291\n",
       "the       1.000000\n",
       "Name: idf, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building TF matrix with Counter and computing IDF vector\n",
    "from collections import Counter\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_tf_matrix(corpus, normalize=False):\n",
    "    \"\"\"\n",
    "    corpus: list of strings (documents) or list of list of tokens\n",
    "    normalize: if True, convert counts to term-frequency (count / total_terms_in_doc)\n",
    "    returns: pandas DataFrame (documents x terms) with TF counts or TF normalized values\n",
    "    \"\"\"\n",
    "    # tokenize if needed (simple whitespace tokenize)\n",
    "    tokenized = []\n",
    "    for doc in corpus:\n",
    "        if isinstance(doc, str):\n",
    "            toks = doc.split()\n",
    "        else:\n",
    "            toks = list(doc)\n",
    "        tokenized.append(toks)\n",
    "    # build vocabulary (sorted for stable order)\n",
    "    vocab = sorted({tok for doc in tokenized for tok in doc})\n",
    "    # build tf rows using Counter\n",
    "    rows = []\n",
    "    for toks in tokenized:\n",
    "        c = Counter(toks)\n",
    "        if normalize:\n",
    "            total = sum(c.values()) or 1\n",
    "            row = [c.get(term, 0)/total for term in vocab]\n",
    "        else:\n",
    "            row = [c.get(term, 0) for term in vocab]\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows, columns=vocab)\n",
    "    return df\n",
    "\n",
    "def compute_idf(corpus, smooth=True, add_one=True, log_base=math.e):\n",
    "    \"\"\"\n",
    "    corpus: list of strings or list of token lists\n",
    "    smooth: if True use smoothing (idf = log((N+1)/(df+1)) + 1)\n",
    "            if False use idf = log(N/df)\n",
    "    add_one: if True add 1 to idf result (common variants)\n",
    "    log_base: base for logarithm (default natural log)\n",
    "    returns: pandas Series indexed by term with idf values\n",
    "    \"\"\"\n",
    "    # tokenization\n",
    "    tokenized = []\n",
    "    for doc in corpus:\n",
    "        if isinstance(doc, str):\n",
    "            toks = set(doc.split())  # use set to compute document frequency\n",
    "        else:\n",
    "            toks = set(doc)\n",
    "        tokenized.append(toks)\n",
    "    N = len(tokenized)\n",
    "    # collect document frequencies\n",
    "    df_counts = Counter()\n",
    "    for toks in tokenized:\n",
    "        for t in toks:\n",
    "            df_counts[t] += 1\n",
    "    vocab = sorted(df_counts.keys())\n",
    "    idf_vals = []\n",
    "    for t in vocab:\n",
    "        df_t = df_counts[t]\n",
    "        if smooth:\n",
    "            val = math.log((N + 1) / (df_t + 1), log_base) + 1\n",
    "        else:\n",
    "            val = math.log(N / df_t, log_base)\n",
    "            if add_one:\n",
    "                val = val + 1\n",
    "        idf_vals.append(val)\n",
    "    return pd.Series(idf_vals, index=vocab, name=\"idf\")\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"the quick brown fox\",\n",
    "    \"jumped over the lazy dog\",\n",
    "    \"the fox\",\n",
    "    \"the dog dog dog\",\n",
    "]\n",
    "\n",
    "tf_counts = build_tf_matrix(corpus, normalize=False)\n",
    "tf_normalized = build_tf_matrix(corpus, normalize=True)\n",
    "idf_series = compute_idf(corpus, smooth=True)\n",
    "\n",
    "display(tf_counts)\n",
    "display(tf_normalized)\n",
    "display(idf_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750c14f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
