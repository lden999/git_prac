{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch torchtext numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-10T09:51:45.011836Z",
     "iopub.status.busy": "2025-09-10T09:51:45.011555Z",
     "iopub.status.idle": "2025-09-10T09:51:45.127772Z",
     "shell.execute_reply": "2025-09-10T09:51:45.127254Z",
     "shell.execute_reply.started": "2025-09-10T09:51:45.011818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import requests\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T09:51:47.559532Z",
     "iopub.status.busy": "2025-09-10T09:51:47.558743Z",
     "iopub.status.idle": "2025-09-10T09:51:47.568650Z",
     "shell.execute_reply": "2025-09-10T09:51:47.568109Z",
     "shell.execute_reply.started": "2025-09-10T09:51:47.559506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, texts, word_to_idx, window_size=2):\n",
    "        self.data = []\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        for text in texts:\n",
    "            tokens = self._preprocess_text(text)\n",
    "            for i in range(len(tokens)):\n",
    "                target_word = tokens[i]\n",
    "                if target_word in word_to_idx:\n",
    "                    context = []\n",
    "                    for j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n",
    "                        if j != i and tokens[j] in word_to_idx:\n",
    "                            context.append(word_to_idx[tokens[j]])\n",
    "                    \n",
    "                    if len(context) >= 2:\n",
    "                        while len(context) < 4:\n",
    "                            context.append(0)\n",
    "                        self.data.append((context[:4], word_to_idx[target_word]))\n",
    "    \n",
    "    def _preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        tokens = text.split()\n",
    "        return [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        context_tensor = torch.tensor(context, dtype=torch.long)\n",
    "        return context_tensor, torch.tensor(target, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T09:51:49.170030Z",
     "iopub.status.busy": "2025-09-10T09:51:49.169740Z",
     "iopub.status.idle": "2025-09-10T09:51:49.175082Z",
     "shell.execute_reply": "2025-09-10T09:51:49.174535Z",
     "shell.execute_reply.started": "2025-09-10T09:51:49.170009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, context):\n",
    "        embeds = self.embedding(context)\n",
    "        context_vector = torch.mean(embeds, dim=1)\n",
    "        hidden = F.relu(self.linear1(context_vector))\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.linear2(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T09:57:20.060288Z",
     "iopub.status.busy": "2025-09-10T09:57:20.059604Z",
     "iopub.status.idle": "2025-09-10T09:57:48.064641Z",
     "shell.execute_reply": "2025-09-10T09:57:48.063914Z",
     "shell.execute_reply.started": "2025-09-10T09:57:20.060263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def download_imdb_data():\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    \n",
    "    try:\n",
    "        print(\"Downloading IMDB dataset...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(\"aclImdb_v1.tar.gz\", \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            import tarfile\n",
    "            with tarfile.open(\"aclImdb_v1.tar.gz\", \"r:gz\") as tar:\n",
    "                tar.extractall()\n",
    "            \n",
    "            texts = []\n",
    "            import glob\n",
    "            \n",
    "            pos_files = glob.glob(\"aclImdb/train/pos/*.txt\")\n",
    "            neg_files = glob.glob(\"aclImdb/train/neg/*.txt\")\n",
    "            \n",
    "            for file_path in pos_files + neg_files:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    texts.append(f.read())\n",
    "            \n",
    "            print(f\"Loaded {len(texts)} IMDB reviews\")\n",
    "            return texts\n",
    "    except Exception as e:\n",
    "        print(f\"Could not download IMDB data: {e}\")\n",
    "        \n",
    "imdb_texts = download_imdb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T09:57:51.424799Z",
     "iopub.status.busy": "2025-09-10T09:57:51.424278Z",
     "iopub.status.idle": "2025-09-10T09:57:53.497257Z",
     "shell.execute_reply": "2025-09-10T09:57:53.496580Z",
     "shell.execute_reply.started": "2025-09-10T09:57:51.424773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return [token for token in tokens if len(token) > 1]\n",
    "\n",
    "word_counts = Counter()\n",
    "for text in imdb_texts:\n",
    "    tokens = preprocess_text(text)\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "min_freq = 3\n",
    "vocab_words = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "\n",
    "word_to_idx = {'<PAD>': 0}\n",
    "idx_to_word = {0: '<PAD>'}\n",
    "\n",
    "for i, word in enumerate(vocab_words, 1):\n",
    "    word_to_idx[word] = i\n",
    "    idx_to_word[i] = word\n",
    "\n",
    "vocab = set(vocab_words)\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample words: {list(vocab)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T09:57:54.018630Z",
     "iopub.status.busy": "2025-09-10T09:57:54.018375Z",
     "iopub.status.idle": "2025-09-10T09:57:54.024030Z",
     "shell.execute_reply": "2025-09-10T09:57:54.023211Z",
     "shell.execute_reply.started": "2025-09-10T09:57:54.018611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T09:57:55.598730Z",
     "iopub.status.busy": "2025-09-10T09:57:55.598487Z",
     "iopub.status.idle": "2025-09-10T09:57:55.761303Z",
     "shell.execute_reply": "2025-09-10T09:57:55.760638Z",
     "shell.execute_reply.started": "2025-09-10T09:57:55.598713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "window_size = 5\n",
    "\n",
    "model = CBOWModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "print(\"Model initialized\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T09:58:05.341680Z",
     "iopub.status.busy": "2025-09-10T09:58:05.340957Z",
     "iopub.status.idle": "2025-09-10T09:58:23.339782Z",
     "shell.execute_reply": "2025-09-10T09:58:23.339131Z",
     "shell.execute_reply.started": "2025-09-10T09:58:05.341645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = CBOWDataset(imdb_texts, word_to_idx, window_size)\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} training samples\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T09:58:26.337498Z",
     "iopub.status.busy": "2025-09-10T09:58:26.336938Z",
     "iopub.status.idle": "2025-09-10T10:08:29.058823Z",
     "shell.execute_reply": "2025-09-10T10:08:29.057798Z",
     "shell.execute_reply.started": "2025-09-10T09:58:26.337476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (context, target) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        context = context.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(context)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if batch_idx % 1000 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f'Epoch {epoch + 1}/{epochs} completed, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Set, Dict, Tuple\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9']+\", \" \", text)\n",
    "    return [t for t in text.split() if t]\n",
    "\n",
    "def kgrams(term: str, k: int = 2, use_boundaries: bool = False) -> Set[str]:\n",
    "    t = ('^' + term + '$') if use_boundaries else term\n",
    "    if len(t) < k:\n",
    "        return {t}\n",
    "    return {t[i:i+k] for i in range(len(t)-k+1)}\n",
    "\n",
    "def build_kgram_index_from_docs(documents: List[str], k: int = 2, use_boundaries: bool = False) -> Tuple[Dict[str, Set[str]], Set[str]]:\n",
    "    vocab = set()\n",
    "    for doc in documents:\n",
    "        vocab.update(tokenize(doc))\n",
    "    index = defaultdict(set)\n",
    "    for term in vocab:\n",
    "        for kg in kgrams(term, k, use_boundaries):\n",
    "            index[kg].add(term)\n",
    "    return dict(index), vocab\n",
    "\n",
    "def generate_candidates_from_index(query_word: str, index: Dict[str, Set[str]], k: int = 2, use_boundaries: bool = False, max_candidates: int = 1000) -> Set[str]:\n",
    "    q_k = kgrams(query_word, k, use_boundaries)\n",
    "    candidates = set()\n",
    "    for kg in q_k:\n",
    "        candidates.update(index.get(kg, ()))\n",
    "        if len(candidates) >= max_candidates:\n",
    "            break\n",
    "    candidates.discard(query_word)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T10:10:25.386533Z",
     "iopub.status.busy": "2025-09-10T10:10:25.385922Z",
     "iopub.status.idle": "2025-09-10T10:10:25.395629Z",
     "shell.execute_reply": "2025-09-10T10:10:25.395084Z",
     "shell.execute_reply.started": "2025-09-10T10:10:25.386511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_candidates(word, vocab):\n",
    "    def edits1(word):\n",
    "        letters = string.ascii_lowercase\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "    \n",
    "    def known(words):\n",
    "        return set(w for w in words if w in vocab)\n",
    "    \n",
    "    candidates = known([word])\n",
    "    if not candidates:\n",
    "        candidates = known(edits1(word))\n",
    "    if not candidates:\n",
    "        edits2 = set(e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "        candidates = known(edits2)\n",
    "    if not candidates:\n",
    "        candidates = {word}\n",
    "    \n",
    "    return list(candidates)\n",
    "\n",
    "def get_context_score(model, word_to_idx, context_words, candidate):\n",
    "    if candidate not in word_to_idx:\n",
    "        return float('-inf')\n",
    "    \n",
    "    context_indices = [word_to_idx.get(word, 0) for word in context_words if word in word_to_idx]\n",
    "    \n",
    "    if len(context_indices) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    while len(context_indices) < 4:\n",
    "        context_indices.append(0)\n",
    "    \n",
    "    context_tensor = torch.tensor([context_indices[:4]], dtype=torch.long)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        context_tensor = context_tensor.to(device)\n",
    "        output = model(context_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        probabilities = probabilities.to('cpu')\n",
    "        candidate_idx = word_to_idx[candidate]\n",
    "        score = probabilities[0, candidate_idx].item()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T10:10:26.043232Z",
     "iopub.status.busy": "2025-09-10T10:10:26.042699Z",
     "iopub.status.idle": "2025-09-10T10:10:26.048692Z",
     "shell.execute_reply": "2025-09-10T10:10:26.048151Z",
     "shell.execute_reply.started": "2025-09-10T10:10:26.043210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def correct_spelling(sentence, model, word_to_idx, vocab, window_size=2):\n",
    "    words = preprocess_text(sentence)\n",
    "    corrected_words = []\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word in vocab:\n",
    "            corrected_words.append(word)\n",
    "        else:\n",
    "            candidates = generate_candidates(word, vocab)\n",
    "            \n",
    "            context_words = []\n",
    "            start_idx = max(0, i - window_size)\n",
    "            end_idx = min(len(words), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start_idx, end_idx):\n",
    "                if j != i and j < len(words):\n",
    "                    context_words.append(words[j])\n",
    "            \n",
    "            best_candidate = word\n",
    "            best_score = float('-inf')\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                score = get_context_score(model, word_to_idx, context_words, candidate)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_candidate = candidate\n",
    "            \n",
    "            corrected_words.append(best_candidate)\n",
    "    \n",
    "    return ' '.join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T10:16:25.299110Z",
     "iopub.status.busy": "2025-09-10T10:16:25.298312Z",
     "iopub.status.idle": "2025-09-10T10:16:25.302925Z",
     "shell.execute_reply": "2025-09-10T10:16:25.302276Z",
     "shell.execute_reply.started": "2025-09-10T10:16:25.299081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def interactive_correction(sentence):\n",
    "    corrected = correct_spelling(sentence, model, word_to_idx, vocab, window_size)\n",
    "    print(f\"Original:  {sentence}\")\n",
    "    print(f\"Corrected: {corrected}\")\n",
    "    return corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T10:38:11.849222Z",
     "iopub.status.busy": "2025-09-10T10:38:11.848958Z",
     "iopub.status.idle": "2025-09-10T10:38:11.940400Z",
     "shell.execute_reply": "2025-09-10T10:38:11.939815Z",
     "shell.execute_reply.started": "2025-09-10T10:38:11.849203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "custom_sentence = \"ahmdabad form delh\"\n",
    "result = interactive_correction(custom_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
