{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2013,
     "status": "ok",
     "timestamp": 1759810668630,
     "user": {
      "displayName": "PATEL SHIV",
      "userId": "18221086023291031373"
     },
     "user_tz": -330
    },
    "id": "iVve5Fg2JFr4",
    "outputId": "be27dc02-ebb7-451b-df17-c41500c71841"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The quick brown fox jumped over the lazy dog\",\n",
    "    \"Amazon is a big e-commerce company\",\n",
    "]\n",
    "\n",
    "query = \"fox jumped\"\n",
    "\n",
    "tokenized_documents = [doc.lower().split() for doc in documents]\n",
    "tokenized_query = query.lower().split()\n",
    "\n",
    "vocabulary = sorted(list(set(term for doc in tokenized_documents for term in doc)))\n",
    "vocab_index = {term: i for i, term in enumerate(vocabulary)}\n",
    "\n",
    "def calculate_tf(tokenized_docs, vocab):\n",
    "    tf_vectors = []\n",
    "    for doc in tokenized_docs:\n",
    "        doc_counts = Counter(doc)\n",
    "        total_terms = len(doc)\n",
    "        tf_vector = [doc_counts.get(term, 0) / total_terms for term in vocab]\n",
    "        tf_vectors.append(tf_vector)\n",
    "    return np.array(tf_vectors)\n",
    "\n",
    "tf_matrix = calculate_tf(tokenized_documents, vocabulary)\n",
    "\n",
    "query_tf_counts = Counter(tokenized_query)\n",
    "query_tf = np.array([query_tf_counts.get(term, 0) / len(tokenized_query) for term in vocabulary])\n",
    "\n",
    "\n",
    "def calculate_idf(tokenized_docs, vocab):\n",
    "    num_documents = len(tokenized_docs)\n",
    "\n",
    "    doc_freq = np.zeros(len(vocab))\n",
    "    for term_index, term in enumerate(vocab):\n",
    "\n",
    "        for doc in tokenized_docs:\n",
    "            if term in doc:\n",
    "                doc_freq[term_index] += 1\n",
    "\n",
    "    idf_vector = [math.log(num_documents / (df + 1)) + 1 for df in doc_freq]\n",
    "    return np.array(idf_vector)\n",
    "\n",
    "idf_vector = calculate_idf(tokenized_documents, vocabulary)\n",
    "\n",
    "tfidf_matrix = tf_matrix * idf_vector\n",
    "query_tfidf = query_tf * idf_vector\n",
    "\n",
    "def cosine_similarity_scratch(vec1, vec2):\n",
    "    \"\"\"Calculates the cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "cosine_sim = [cosine_similarity_scratch(query_tfidf, doc_tfidf) for doc_tfidf in tfidf_matrix]\n",
    "\n",
    "ranked_indices = np.argsort(-np.array(cosine_sim))[:5]\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Rank\": range(1, 6),\n",
    "    \"Document\": [documents[i] for i in ranked_indices],\n",
    "    \"Score\": [cosine_sim[i] for i in ranked_indices]\n",
    "})\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nTop 5 Matching Documents:\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZBVzq7EN6Z5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPrIV1AWy27618FtS0CAcWq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
