{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T09:45:12.030927Z",
     "iopub.status.busy": "2025-08-13T09:45:12.030598Z",
     "iopub.status.idle": "2025-08-13T09:45:12.043525Z",
     "shell.execute_reply": "2025-08-13T09:45:12.042801Z",
     "shell.execute_reply.started": "2025-08-13T09:45:12.030901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-13T09:45:12.221800Z",
     "iopub.status.busy": "2025-08-13T09:45:12.221501Z",
     "iopub.status.idle": "2025-08-13T09:45:17.026207Z",
     "shell.execute_reply": "2025-08-13T09:45:17.025377Z",
     "shell.execute_reply.started": "2025-08-13T09:45:12.221778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\",header = \"infer\")\n",
    "df = df[['Id','Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T09:45:17.028154Z",
     "iopub.status.busy": "2025-08-13T09:45:17.027808Z",
     "iopub.status.idle": "2025-08-13T09:45:17.033537Z",
     "shell.execute_reply": "2025-08-13T09:45:17.032712Z",
     "shell.execute_reply.started": "2025-08-13T09:45:17.028123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T09:45:17.035190Z",
     "iopub.status.busy": "2025-08-13T09:45:17.034538Z",
     "iopub.status.idle": "2025-08-13T09:45:17.168338Z",
     "shell.execute_reply": "2025-08-13T09:45:17.167578Z",
     "shell.execute_reply.started": "2025-08-13T09:45:17.035168Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "reviews = list(df['Text'])[:2000]\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T09:45:17.170280Z",
     "iopub.status.busy": "2025-08-13T09:45:17.169954Z",
     "iopub.status.idle": "2025-08-13T09:45:23.093056Z",
     "shell.execute_reply": "2025-08-13T09:45:23.092134Z",
     "shell.execute_reply.started": "2025-08-13T09:45:17.170250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    return [word_tokenize(doc) for doc in corpus]\n",
    "\n",
    "def case_fold_corpus(tokenized_doc):\n",
    "    return [word.lower() for word in tokenized_doc]\n",
    "\n",
    "def get_stop_words():\n",
    "    return set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(corpus, stop_words):\n",
    "    return [[word for word in doc if word.isalpha() and word not in stop_words] for doc in corpus]\n",
    "\n",
    "def stem_corpus(tokenized_doc):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokenized_doc]\n",
    "\n",
    "def pos_tag_corpus(tokenized_doc):\n",
    "    return pos_tag(tokenized_doc)\n",
    "\n",
    "def lemmatize_corpus(pos_tagged_doc):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tagged_doc]\n",
    "\n",
    "# Main processing\n",
    "def process_corpus(corpus):\n",
    "    tokenized_corpus = tokenize_corpus(corpus)\n",
    "    tokenized_corpus = [case_fold_corpus(doc) for doc in tokenized_corpus]\n",
    "    stop_words = get_stop_words()\n",
    "    no_stop_corpus = remove_stop_words(tokenized_corpus, stop_words)\n",
    "    stemmed_corpus = [stem_corpus(doc) for doc in no_stop_corpus]\n",
    "    pos_tagged_corpus = [pos_tag_corpus(doc) for doc in no_stop_corpus]\n",
    "    lemmatized_corpus = [lemmatize_corpus(doc) for doc in pos_tagged_corpus]\n",
    "\n",
    "    lemmatized_texts = [' '.join(tokens) for tokens in lemmatized_corpus]\n",
    "\n",
    "    return {\n",
    "        'tokenized': tokenized_corpus,\n",
    "        'no_stop': no_stop_corpus,\n",
    "        'stemmed': stemmed_corpus,\n",
    "        'pos_tagged': pos_tagged_corpus,\n",
    "        'lemmatized_tokens': lemmatized_corpus, \n",
    "        'lemmatized_texts': lemmatized_texts     \n",
    "    }\n",
    "\n",
    "\n",
    "d = process_corpus(reviews)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T09:45:31.426006Z",
     "iopub.status.busy": "2025-08-13T09:45:31.425710Z",
     "iopub.status.idle": "2025-08-13T09:45:31.431779Z",
     "shell.execute_reply": "2025-08-13T09:45:31.430654Z",
     "shell.execute_reply.started": "2025-08-13T09:45:31.425985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_frequent_nouns_from_list(text_list, threshold=10):\n",
    "    noun_list = []\n",
    "    for text in text_list:\n",
    "        words = text.split()\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        for word, tag in tagged:\n",
    "            if tag.startswith(\"NN\"):\n",
    "                noun_list.append(word.lower())\n",
    "    noun_freq = Counter(noun_list)\n",
    "    frequent_nouns = {noun for noun, count in noun_freq.items() if count >= threshold}\n",
    "    print(f\"Found {len(frequent_nouns)} frequent nouns with threshold {threshold}\\n\")\n",
    "    return frequent_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T09:45:31.635231Z",
     "iopub.status.busy": "2025-08-13T09:45:31.634519Z",
     "iopub.status.idle": "2025-08-13T09:45:31.645213Z",
     "shell.execute_reply": "2025-08-13T09:45:31.644365Z",
     "shell.execute_reply.started": "2025-08-13T09:45:31.635194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_noun_modifiers_from_list(text_list, frequent_nouns, window_size=2):\n",
    "    results = {}\n",
    "    for text in text_list:\n",
    "        words = text.split()\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        for i, (word, tag) in enumerate(tagged):\n",
    "            noun = word.lower()\n",
    "            if tag.startswith(\"NN\") and noun in frequent_nouns:\n",
    "                modifiers = []\n",
    "                for j in range(max(0, i - window_size), min(len(tagged), i + window_size + 1)):\n",
    "                    if j == i:\n",
    "                        continue\n",
    "                    mod_word, mod_tag = tagged[j]\n",
    "                    if mod_tag.startswith(\"JJ\"):\n",
    "                        modifiers.append(mod_word.lower())\n",
    "                    elif mod_tag.startswith(\"RB\"):\n",
    "                        if j + 1 < len(tagged) and tagged[j + 1][1].startswith(\"JJ\"):\n",
    "                            modifiers.append(f\"{mod_word.lower()} {tagged[j + 1][0].lower()}\")\n",
    "                        else:\n",
    "                            modifiers.append(mod_word.lower())\n",
    "                if modifiers:\n",
    "                    if noun not in results:\n",
    "                        results[noun] = []\n",
    "                    results[noun].extend(modifiers)\n",
    "    return results\n",
    "\n",
    "def print_noun_modifiers(results, top_n=15, top_mods=5):\n",
    "    for noun, mods in list(results.items())[:top_n]:\n",
    "        mod_counts = Counter(mods)\n",
    "        top_mod_list = ', '.join([f\"{mod} ({count})\" for mod, count in mod_counts.most_common(top_mods)])\n",
    "        print(f\"{noun}: {top_mod_list}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T09:46:45.144635Z",
     "iopub.status.busy": "2025-08-13T09:46:45.144372Z",
     "iopub.status.idle": "2025-08-13T09:46:57.937656Z",
     "shell.execute_reply": "2025-08-13T09:46:57.936890Z",
     "shell.execute_reply.started": "2025-08-13T09:46:45.144617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processed = process_corpus(reviews)\n",
    "\n",
    "frequent_nouns = get_frequent_nouns_from_list(processed['lemmatized_texts'], threshold=20)\n",
    "\n",
    "results = find_noun_modifiers_from_list(processed['lemmatized_texts'], frequent_nouns, window_size=2)\n",
    "\n",
    "print_noun_modifiers(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1461623,
     "sourceId": 2415872,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
